{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4758d1bc",
   "metadata": {},
   "source": [
    "# LMA: Levenberg-Marquardt Algorithm\n",
    "\n",
    "The Levenberg-Marquardt algorithm is an iterative procedure widely used for solving non-linear least squares problems or for finding roots of non-linear systems of equations. This implementation is designed to be robust and offer maximum flexibility, but at the same time provides sensible defaults to facilitate its usage.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The LMA method interpolates between the more aggressive Gauss-Newton algorithm and the more conservative method of gradient descent to find the set of *solution parameters* $p$ that minimize a cost function $F(p)$, defined as:\n",
    "\n",
    "$$F(p) = \\sum_i \\rho(y_i(p), c_i)$$\n",
    "\n",
    "where $y_i(p)$ are the components of the residuals vector $y$, and $\\rho$ is a loss function which depends on the residuals and a scaling factor $c_i$. In the standard case ($L_2$ loss function), $F(p)$ is the sum of squared residuals (and $c_i = 1$).\n",
    "\n",
    "Every iteration of the algorithm, the residuals and the Jacobian are calculated for a given set of parameters $p$. Then, a new guess $p-\\Delta p$ is calculated such that:\n",
    "\n",
    "$$(J^T W J + \\lambda D)\\Delta p = J^T W y$$\n",
    "\n",
    "where $J$ is the Jacobian of the residuals ($J_{ij}=\\partial y_i / \\partial p_j$), $W$ is a weight matrix depending on the choice of loss function (the identity matrix for $L_2$), $D$ is a diagonal matrix such that $D_{kk} = \\max(\\epsilon(\\lambda), (J^T W J)_{kk})$ with $\\epsilon(\\lambda)$ the damping floor (dependent on the damping factor), $y$ is the residuals vector, and $\\lambda$ is the damping factor. The damping factor determines how much the next guess approximates the prediction of the Gauss-Newton algorithm (lower damping factors) or the gradient descent methods (higher damping factors), in effect defining a trust-region.\n",
    "\n",
    "For this new guess, the predicted error reduction is calculated as:\n",
    "\n",
    "$$\\Delta s_p = \\frac{1}{2}(\\Delta p^T J^T W y + \\lambda \\Delta p^T D \\Delta p)$$\n",
    "\n",
    "If the ratio of the actual error reduction calculated during the next iteration with respect to this prediction is above a defined limit, the current guess is accepted and the damping factor is decreased. Else, the new guess is rejected and the damping factor is increased.\n",
    "\n",
    "The algorithm finishes once one of these conditions is met, returning the last accepted guess:\n",
    "\n",
    "* Maximum number of iterations reached\n",
    "* Cost (sum of losses) below specified tolerance\n",
    "* Relative change in solution parameters or cost below specified tolerance\n",
    "* Stagnation at maximum damping factor\n",
    "\n",
    "### Choice of loss function\n",
    "\n",
    "The standard $L_2$ loss function calculates the loss as the square of the residual. Its corresponding weight matrix is the identity. Moreover, if an individual scaling factor is defined for each residual, this loss function can be used for the solution of *weighted least-squares* problems.\n",
    "\n",
    "Robust loss functions provide mechanisms to mitigate the effect of outliers in fitting data:\n",
    "\n",
    "* **Huber** Equivalent to $L_2$ for small residuals and linear for larger residuals\n",
    "* **Cauchy** Strongly down-weights large outliers\n",
    "* **L1-Soft** Smooth approximation of $L_1$ loss that behaves like $L_2$ for small residuals\n",
    "* **Tukey** Redescending M-estimator that completely rejects extreme outliers (but it may lead to convergence issues if scaling is not chosen well)\n",
    "* **Welsh** Another redescending M-estimator, smoother than Tukey's in its rejection\n",
    "* **Fair** Less sensitive to large errors than $L_2$, but not redescending\n",
    "* **Arctan** Limits maximum loss of single residuals\n",
    "\n",
    "### Normalized damping factor\n",
    "\n",
    "A particularity of this LMA implementation is the use of a *normalized damping factor*. The damping factor $\\lambda$ will oscillate between the specified limits $\\lambda_{min}$ and $\\lambda_{max}$, starting with an initial value $\\lambda_0$. The normalized damping factor is calculated as:\n",
    "\n",
    "$$\\lambda_{norm} = \\frac{(\\lambda_{max}-\\lambda_0)(\\lambda-\\lambda_{min})}{(\\lambda_0-\\lambda_{min})(\\lambda_{max}-\\lambda)}$$\n",
    "\n",
    "If $\\lambda_{norm}$ is 1, the initial damping factor is being used. If it is larger, it means that more damping than indicated was necessary, reaching the maximum $\\lambda_{max}$ at infinite; if it is lower, it means that it could be decreased for faster convergence, with the point at 0 corresponding with $\\lambda_{min}$.\n",
    "\n",
    "The usage of normalized damping factors allows to monitor and adjust the effective size of the trust region during successive function calls independently of the actual damping parameters at use.\n",
    "\n",
    "### Adaptive damping floor\n",
    "\n",
    "To enhance numerical stability, particularly when dealing with Jacobian matrices $J$ where $J^T W J$ may have very small or zero diagonal elements, this LMA implementation employs an adaptive floor for the diagonal elements of the damping scaling matrix $D$. The floor $\\epsilon(\\lambda)$ is calculated such that it takes the value $\\epsilon_0$ (an arbitrarily small number) when $\\lambda$ is $\\lambda_0$ or lower than $\\lambda_0$ (so, $\\lambda_{norm}≤1$), and it approaches 1 as $\\lambda$ approaches $\\lambda_{max}$.\n",
    "\n",
    "This adaptive mechanism ensures that while a very low floor is used during optimistic (low damping) phases, a more substantial floor (approaching 1) is automatically applied to weak components when high overall damping is required.\n",
    "\n",
    "## Usage\n",
    "\n",
    "### `Jacobian` Operator\n",
    "\n",
    "    R←{X}f Jacobian Y\n",
    "    \n",
    "`Jacobian` is a monadic operator that takes a monadic function `f` as left operand to return an ambivalent function. This derived function returns an estimation of the Jacobian matrix of `f`, using the method of finite differences. The right argument `Y` is the value at which the Jacobian is calculated, and the optional left argument `X` is the relative perturbation to apply to `Y` in the finite differences method. If `X` is not a given, `⎕CT*÷2` is used.\n",
    "\n",
    "### `LMA` Operator\n",
    "\n",
    "    R←{X}f LMA Y\n",
    "\n",
    "`LMA` is a monadic operator, which takes a left operand to return a derived ambivalent function. This derived function allows to minimize a residual function with a known Jacobian using the Levenberg-Marquardt algorithm, given an initial set of parameters. Several configuration options are available, with sensible defaults previously defined.\n",
    "\n",
    "The left operand `f` must be a configuration namespace or a function. Configuration namespaces may define the following configuration options:\n",
    "\n",
    "* `toli`: Maximum number of iterations (default `1E3`)\n",
    "* `tolc`: Tolerance for the cost (sum of squared residuals or loss values) (default `⎕CT`)\n",
    "* `tolr`: Tolerance for relative change, either in the solution or the residual (default `⎕CT`)\n",
    "* `tolg`: Tolerance for the gain ratio to accept or reject a step (default `1E¯2`)\n",
    "* `dini`: Initial damping factor for `dnorm=1` (default `1E¯2`)\n",
    "* `dinc`: Increment of damping factor after rejected solution (default `5`)\n",
    "* `ddec`: Decrement of damping factor after accepted solution (default `÷dinc`)\n",
    "* `dmax`: Maximum damping factor (default `÷⎕CT`)\n",
    "* `dmin`: Minimum damping factor (default `÷dmax`)\n",
    "* `pert`: Relative perturbation applied to parameters for numerical estimation of the Jacobian (default `⎕CT*÷2`)\n",
    "* `loss`: Choice of loss function: `L2` `Huber` `Cauchy` `L1Soft` `Tukey` `Welsh` `Fair` `Arctan` or dyadic function (default `L2`)\n",
    "* `scale`: Scale factor passed as left argument to loss function (default for 95% efficiency in robust loss functions)\n",
    "* `verbose`: If `1`, print `iter cost rel dnorm p` each iteration (default `0`)\n",
    "\n",
    "Configuration namespaces may also contain the functions:\n",
    "\n",
    "* `Callback`: Callback function (default `⊢`)\n",
    "* `Eval`: Evaluation function\n",
    "\n",
    "The evaluation function `Eval` must return either the residuals and the Jacobian for the given set of solution parameters, or only the residuals. Whenever the residual and Jacobian need to be evaluated, the function `Eval` will be called with trial parameters as right argument and left argument `X`, if given (`Eval` will be called monadically if the derived function `f LMA` is called monadically). `Eval` must return either a two elements vector with the residuals in the first element and the Jacobian in the second one, or a vector of residuals, enclosed if they are not simple scalars. If a Jacobian is not returned, a numerical estimation is calculated evaluating the residual function after applying small perturbations to the parameters (as defined by `pert`).\n",
    "\n",
    "The function selected by the option `loss` is used to calculate the loss from the residuals and scaling factor. If a function is provided by the user, it must be a dyadic function which returns the loss values and weights when given the residuals as right argument and scaling factor as left argument.\n",
    "\n",
    "The `Callback` function will be called every iteration before checking convergence, with the current solution namespace as right argument and `X` as left argument, if given (`Callback` will be called monadically if the derived function `f LMA` is called monadically). Its return value is discarded.\n",
    "\n",
    "If `f` is a function, the result is equivalent to using as `f` a namespace with an `Eval` function `f`\n",
    "(with default values for the rest of parameters).\n",
    "\n",
    "`Y` must be a vector.\n",
    "The first element of `Y`, or `⊂Y` if `1=≡Y`, contains the initial guess for the solution parameters.\n",
    "If the next element of `Y` is a scalar numeric value, it is interpreted as the initial normalized damping factor.\n",
    "Additional elements of `Y` must be configuration namespaces. The final configuration parameters are obtained\n",
    "overwriting the parameters in the namespace given as left operand with those given as right argument from right to left. Default values will be used for non-defined parameters and the `Callback` function, but the `Eval` function must be defined by the user either as left operand `f` or as member of a configuration namespace.\n",
    "\n",
    "The returned value `R` is a solution namespace corresponding to the last accepted solution.\n",
    "A solution namespace is a configuration namespace including all the configuration options used to run the algorithm and the additional elements:\n",
    "\n",
    "* `iter`: Number of iterations\n",
    "* `cost`: Sum of loss values (squared residuals for L2)\n",
    "* `rel`: Relative change metric\n",
    "* `dnorm`: Normalized damping factor\n",
    "* `p0`: Initial guess\n",
    "* `p`: Accepted guess\n",
    "\n",
    "#### Notes\n",
    "\n",
    "* With the exception of `toli` and `tolc`, configuration parameters should be modified only by expert users or in case of convergence problems\n",
    "\n",
    "* The relative change metric `rel` is the minimum relative change between successive accepted solutions either in the the cost or in the solution parameters\n",
    "\n",
    "* In addition to being used for the definition of default values, `⎕CT` is also the baseline for adaptive floor damping\n",
    "\n",
    "* The perturbation to estimate the Jacobian `pert` and the scaling factor for loss functions `scale` can be either scalar values, or vectors of the same length of respectively the parameters and the residuals\n",
    "\n",
    "* Loss functions and their respective weights, as well as their default values for the scaling parameter are defined in the namespace `Loss`\n",
    "\n",
    "\n",
    "### `LM` Operator\n",
    "\n",
    "`LM` is a simplified version of `LMA`. It is a dyadic operator from which a dyadic function is derived. Usage:\n",
    "\n",
    "    R←X f LM g Y\n",
    "\n",
    "where `f` is a monadic evaluation function, `g` is a monadic function which takes as argument an `iter cost rel dnorm p` vector and gets called before every convergence check, `Y` is a two elements vector with the initial guess of parameters and normalized damping factor, and `X` is a vector with the configuration parameters `toli tolc tolr tolg dini dinc ddec dmax dmin`. The function `f` must return either the residuals (as a simple or nested vector), the residuals and the Jacobian, or the residuals, Jacobian, loss values and weights, for a set of parameters. If no Jacobian is provided, it is estimated numerically using `Jacobian` (with no left argument). If no loss values and weights are provided, squared residuals are used.\n",
    "\n",
    "The return value `R` is an `iter cost rel dnorm p` vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c24e6e",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7cfd778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\">clear ws\n",
       "</pre>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ")clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2759e3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "]dinput\n",
    "Jacobian←{⍺←⎕CT*÷2 ⋄ a←⍺×1@(0∘=)|⍵                     ⍝ Jacobian matrix of ⍺⍺ at ⍵ applying perturbation ⍺\n",
    "    ⍉↑⍺÷⍨(⍺⍺¨(⊂⍵)+↓↑(-⍳≢⍵)↑¨⍺)-⊆⍺⍺ ⍵                   ⍝     finite-differences method\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50175f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "]dinput\n",
    "LM←{ti tc tr tg d0 di dd dx dn←⍺ ⋄ p d←⍵               ⍝ Levenberg–Marquardt algorithm\n",
    "\n",
    "    D←((dx-d0)×dn-⍨⊢)÷(d0-dn)×dx-⊢ ⋄ L←{⍵,(×⍨⊃⍵)2÷2}   ⍝ damping factor normalization(λ) and standard loss(y)\n",
    "    J←{(1<|≡g)∧1<≢g←⍺⍺ ⍵:g ⋄ (⊃⊆g)(⍺⍺ Jacobian ⍵)}     ⍝ residual and (estimated if not given) jacobian(p)\n",
    "    E←⍺⍺{y j l w←L⍣(2=≢g)⊢g←⍺⍺ J ⍵ ⋄ (+/l)⍵ y j w}     ⍝ eval(p): cost, parameters, residual, jacobian\n",
    "    A←{c p y j w←⍵ ⋄ c p(t+.×j)(y+.×⍨t←w×⍤1⍉j)}        ⍝ accept(EG output): sum(error²), parameters, JtJ, Jty\n",
    "    \n",
    "    T←{                                                ⍝ try guess(λ)\n",
    "        r⊢←0 ⋄ 11::dx⌊⍵×di ⋄ b←1-÷1⌈d⊢←D ⍵             ⍝     bad guess if domain error\n",
    "        ∆p←jy⌹jj+⍺×⍤1⊢⍵×dj←(⎕CT+b-⎕CT×b)⌈1 1⍉jj        ⍝     change of parameters with adaptive floor\n",
    "        c0←2÷⍨∆p+.×jy+∆p×⍵×dj                          ⍝     predicted error decrement\n",
    "        c1←c-⊃g←E⊢q←p-∆p                               ⍝     actual error decrement\n",
    "        r⊢←(p(-÷⍥(+.×⍨)⊣)q)⌊|c1÷c                      ⍝     relative change in parameters or residuals\n",
    "        (⎕CT>c0)∧⎕CT<c1:dx⌊⍵×di                        ⍝     if no changing, increase damping\n",
    "        (⎕CT≤c0)∧tg≥c1÷c0:dx⌊⍵×di                      ⍝     if diverging, increase damping\n",
    "        dn⌈⍵×dd⊣c p jj jy⊢←A g                         ⍝     accept change, decrease damping\n",
    "    }\n",
    "    C←⍵⍵{                                              ⍝ convergence check(λ_prev, λ)\n",
    "        _←⍺⍺(i⊢←i+1)c r d p                            ⍝     call user function\n",
    "        (ti<i)∨(dx∧.=⍺ ⍵)∨(tc>c)∨(r>0)∧tr>r            ⍝     iterations, max damping, residual, not changing\n",
    "    }\n",
    "    i r←0 ⋄ 0≥ti⊣⍵⍵ g←i c r d p⊣c p jj jy←A E p:g      ⍝ init\n",
    "    i c r d p⊣(∘.=⍨⍳≢p)T⍣C{11::dx ⋄ D⍣¯1⊢⍵}d           ⍝ iterations, cost, change, norm damping, parameters\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd000a59",
   "metadata": {},
   "outputs": [],
   "source": [
    ":Namespace Loss\n",
    "    l2←1                                               ⍝ standard squared-residual\n",
    "    L2←{(0.5×⍺××⍨⍵)⍺}\n",
    "    \n",
    "    huber←1.345                                        ⍝ ~95% efficiency for normal errors\n",
    "    Huber←{y←⍺>|⍵ ⋄ Y←y⍨ ⋄ N←~Y                        ⍝ L2 for small residuals, lineal for large ones\n",
    "        (((⍺÷2)-⍨⍺×|)@N×⍨@Y ⍵)((⍺÷⍨|)@N 1@Y ⍵)\n",
    "    }\n",
    "    cauchy←2.385                                       ⍝ ~95% efficiency for normal errors\n",
    "    Cauchy←{(⍺×(⍺÷2)×⍟1+r)(÷1+r←×⍨⍵÷⍺)}                ⍝ strongly downweights large outliers\n",
    "    \n",
    "    softl1←1\n",
    "    SoftL1←{(⍺×⍺×r-1)(÷r←(1+×⍨⍵÷⍺)*÷2)}                ⍝ L2 for small residuals, L1 for large ones\n",
    "    \n",
    "    tukey←4.685                                        ⍝ ~95% efficiency for normal errors\n",
    "    Tukey←{k←(×⍨⍺)÷6 ⋄ y←⍺>|⍵ ⋄ Y←y⍨ ⋄ N←~Y            ⍝ re-descending M-estimator\n",
    "        (k@Y(k×1-3*⍨1-⊢)@N⊢r)(0@Y(×⍨1-⊢)@N⊢r←×⍨⍵÷⍺)\n",
    "    }\n",
    "    welsh←2.985                                        ⍝ ~95% efficiency for normal errors\n",
    "    Welsh←{(⍺×(⍺÷2)×1-e)(e←*-×⍨⍵÷⍺)}                   ⍝ smoother re-descending M-estimator\n",
    "    \n",
    "    fair←1\n",
    "    Fair←{(⍺×⍺×r-⍟1+r)(÷1+r←(|⍵)÷⍺)}                   ⍝ no re-descending\n",
    "    \n",
    "    arctan←1\n",
    "    Arctan←{(⍺×(⍺÷2)×¯3○r)(÷1+×⍨r←×⍨⍵÷⍺)}              ⍝ limits maximum loss\n",
    ":EndNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9824af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "]dinput\n",
    "LMA←{⍺←⊢ ⋄ p←⊃w←⊆⍵ ⋄ 0::⎕SIGNAL ⎕EN                    ⍝ pass signals\n",
    "\n",
    "    n←'d',¨'ini' 'inc' 'dec' 'max' 'min'               ⍝ damping\n",
    "    n←('icrg',⍨¨⊂'tol'),n                              ⍝ tolerances\n",
    "    nc←'ddec' 'dmin' ⋄ ns←'scale'                      ⍝ computed defaults\n",
    "    nr←'iter' 'cost' 'rel' 'dnorm' 'p'                 ⍝ results\n",
    "    c←(                                                ⍝ default config\n",
    "        toli:1000 ⋄ tolc:⎕CT ⋄ tolr:⎕CT ⋄ tolg:0.01    ⍝     tolerances: iterations cost relative gain\n",
    "        dini:0.01 ⋄ dinc:5 ⋄ dmax:÷⎕CT ⋄ loss:'L2'     ⍝     damping and loss function\n",
    "        p0:p ⋄ pert:⎕CT*÷2 ⋄ verbose:0                 ⍝     init guess, perturbation, logging\n",
    "    )\n",
    "    F←{1((⊂6 0⍕↑),12 ¯5∘⍕¨⍤↓)⍵} ⋄ P←{⎕←F ⍵ ⋄ ⍵}        ⍝ format and print\n",
    "    D←{⍕'sp',¨':',¨(⊂2 5)⌷F ⍵}                         ⍝ display form\n",
    "    J←{⍉↑⍺÷⍨(⍺⍺¨(⊂⍵)+↓↑(-⍳≢⍵)↑¨⍺)-⊆⍺⍺ ⍵}               ⍝ estimate Jacobian\n",
    "    E←{⍺←⊢ ⋄ (1<≡e)∧2=≢e←⍺⍺ ⍵:e ⋄ (⊃⊆e)(⍺ ⍺⍺ J ⍵)}     ⍝ evaluate, and estimate J if needed\n",
    "    M←(2÷⍨1⊥⊢⌷⍨∘⊂⍋⌷⍨∘⊂∘⌈2÷⍨0 1+≢){1@(0∘=)⍺⍺|⍵-⍺⍺ ⍵}    ⍝ median absolute deviation\n",
    "    L←{0=c.⎕NC'sigma':∇⍵⊣c.sigma←(M ⍵)÷0.6745          ⍝ loss function (needs stddev estimation)\n",
    "        3=⎕NC'⍺⍺':c.(sigma×⎕VGET⊂ns 1)⍺⍺ ⍵             ⍝     user defined\n",
    "        ~(⊂⍺⍺)∊Loss.⎕NL¯3:⎕SIGNAL 6                    ⍝     if no user defined it must be in Loss\n",
    "        c.(sigma×⍣(⍺⍺≢'L2')⊢scale)(Loss.⍎⍺⍺)⍵          ⍝     scaled loss function\n",
    "    }\n",
    "    3=⎕NC'⍺⍺':⍺((⍺⍺{⍵.Eval←⍺⍺ ⋄ ⍵}c)∇∇)w               ⍝ ⍺⍺ is Eval function\n",
    "    (1<≢w)∧~2|⎕DR⊃⌽w:⍺((⎕NS ⍺⍺(⊃⌽w))∇∇)¯1↓w            ⍝ non numeric extra argument is config\n",
    "    2<≢w:⎕SIGNAL 11                                    ⍝ wrong argument\n",
    "    \n",
    "    c.CallBack←⊢ ⋄ c←c ⎕NS ⍺⍺ ⋄ c.dnorm←1⊣⍣(1=≢w)⊃⌽w   ⍝ default callback and actual config\n",
    "    c.scale←c ⎕VGET⊂ns(Loss ⎕VGET ⎕C c.loss)           ⍝ set scale factor for loss function\n",
    "    _←c ⎕VSET(↑nc)(c ⎕VGET(↑nc)c.(÷dinc dmax))         ⍝ other computed defaults\n",
    "    CB←⍺{⍺⍺ c.CallBack c ⎕VSET(↑nr)⍵}P⍣(c.verbose≢0)   ⍝ callback function\n",
    "    EV←⍺∘c.Eval{y j←⍺ ⍺⍺ E ⍵ ⋄ y j,(c.loss L)y}        ⍝ eval function\n",
    "    c⊣c.⎕DF D(c ⎕VGET↑n)(c.pert∘EV)LM CB p c.dnorm     ⍝ return namespace\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54597aa1",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40136c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "]dinput\n",
    "Test←{0::⎕EM ⎕SIGNAL ⎕EN\n",
    "    t←()\n",
    "    ⍝ Tests ability to navigate sharp, narrow valleys and handle situations where the Jacobian\n",
    "    ⍝ may lead to a locally rank-deficient J^T J matrix (eg. zero diagonal elements), requiring\n",
    "    ⍝ robust damping\n",
    "    t.Beale←{\n",
    "        B←{\n",
    "            x y←⍵ ⋄ (1.5 2.25 2.625-x×1-y*⍳3)[(¯1+y)x ⋄ (¯1+y*2)(2×x×y) ⋄ (¯1+y*3)(3×x×y*2)]\n",
    "        }\n",
    "        r←3 0.5 CMP(R B #.LMA 1 0.8).p                                ⍝ easy\n",
    "        r,←3 0.5 CMP(R B #.LMA 1 1).p                                 ⍝ singular Jt J\n",
    "        r,←3 0.5 CMP(R B #.LMA 0 0).p                                 ⍝ another tricky point\n",
    "        r,←3 0.5 CMP(R B #.LMA 1 ¯2).p                                ⍝ different quadrant\n",
    "        r,←(B #.LMA(2 2)(toli:1)).cost>(R B #.LMA 2 2).cost           ⍝ closer to another local minimum\n",
    "        r,←(B #.LMA(¯1 1)(toli:1)).cost>(R B #.LMA ¯1 1).cost         ⍝ closer to another local minimum\n",
    "        r\n",
    "    }\n",
    "    ⍝ numerical approximation of the Jacobian\n",
    "    t.BealeNum←{\n",
    "        B←{\n",
    "            x y←⍵ ⋄ (1.5 2.25 2.625-x×1-y*⍳3)\n",
    "        }\n",
    "        r←3 0.5 CMP(R B #.LMA 1 0.8).p                                ⍝ easy\n",
    "        r,←3 0.5 CMP(R B #.LMA 1 1).p                                 ⍝ singular Jt J\n",
    "        r,←3 0.5 CMP(R B #.LMA 0 0).p                                 ⍝ another tricky point\n",
    "        r,←3 0.5 CMP(R B #.LMA 1 ¯2).p                                ⍝ different quadrant\n",
    "        r,←(B #.LMA(2 2)(toli:1)).cost>(R B #.LMA 2 2).cost           ⍝ closer to another local minimum\n",
    "        r,←(B #.LMA(¯1 1)(toli:1)).cost>(R B #.LMA ¯1 1).cost         ⍝ closer to another local minimum\n",
    "        r\n",
    "    }\n",
    "    ⍝ Jacobian and fitting problem with outliers to test loss functions and bare LM\n",
    "    ExpDec←{A k C←⍺ ⋄ C+A×*-k×⍵}\n",
    "    ExpDecEv←{x y←⍺ ⋄ A k C←⍵ ⋄ xe←A×x×e←*-k×x ⋄ (y-⍵ ExpDec x)(⍉(-e)⍪xe⍪⍉⍪-=⍨e)}\n",
    "    y←100×@(?≢x)⊢0.1{⍵+⍺×0.5-⍨?0⍴⍨≢⍵}y0←(p←10 0.5 1)ExpDec⊢x←(⍳100)-1\n",
    "    t.ExpDecFit←{p x y y0←#.(p x y y0) ⋄ CMP←{0.1>|⍺-⍵}\n",
    "        r←p CMP(R x y0∘#.ExpDecEv #.LMA(10 0.5 1)(loss:'L2')).p                    ⍝ exact\n",
    "        r,←p CMP(R x y0∘#.ExpDecEv #.LMA(10 0.5 1)(loss:'L2' ⋄ scale:?(≢x)⍴0)).p   ⍝ exact (wls)\n",
    "        r,←p CMP(R x y0∘#.ExpDecEv #.LMA(10 0.5 1)(loss:'Huber')).p                ⍝ exact\n",
    "        r,←p CMP(R x y∘#.ExpDecEv #.LMA(5 0.1 0.5)(loss:'Cauchy')).p\n",
    "        r,←p CMP(R x y∘#.ExpDecEv #.LMA(5 0.1 0.5)(loss:'SoftL1')).p\n",
    "        r,←p CMP(R x y0∘#.ExpDecEv #.LMA(5 0.1 0.5)(loss:'Tukey' ⋄ scale:0.1)).p   ⍝ exact with scaling\n",
    "        r,←p CMP(R x y∘#.ExpDecEv #.LMA(5 0.1 0.5)(loss:'Welsh')).p\n",
    "        r,←p CMP(R x y∘#.ExpDecEv #.LMA(5 0.1 0.5)(loss:'Fair')).p\n",
    "        r,←p CMP(R x y∘#.ExpDecEv #.LMA(5 0.1 0.5)(loss:'Arctan')).p\n",
    "        r\n",
    "    }\n",
    "    t.ExpDecJac←{p x y←#.(p x y0) ⋄ tol←1e¯10\n",
    "        R←{⎕←(16↑''),12 ¯5⍕⍵ ⋄ tol>⍵}\n",
    "        r←R+.×⍨,((y-#.ExpDec∘x)#.Jacobian p)-⊃⌽x y #.ExpDecEv p\n",
    "        r,←R+.×⍨,(1e¯9(y-#.ExpDec∘x)#.Jacobian p)-⊃⌽x y #.ExpDecEv p\n",
    "        r\n",
    "    }\n",
    "    t.ExpDecLM←{p x y←#.(p x y0)\n",
    "        R←{⎕←(10↑''),((6 0⍕⊃),(12 ¯5⍕2∘⊃),4∘↓)⍵ ⋄ ⍵}\n",
    "        cfg←1e6 ⎕CT ⎕CT 1e¯2 1e¯2 5(÷5)(÷⎕CT)⎕CT                         ⍝ ti tc tr tg d0 di dd dx dn\n",
    "        r←p CMP⊃⌽R cfg(y-#.ExpDec∘x)#.LM⊢(5 0.1 0.5)1                    ⍝ only residuals\n",
    "        r,←p CMP⊃⌽R cfg(x y∘#.ExpDecEv)#.LM⊢(5 0.1 0.5)1                 ⍝ residual and jacobian\n",
    "        r,←p CMP⊃⌽R cfg((1,⍨⊢,(×⍨1∘↑))x y∘#.ExpDecEv)#.LM⊢(5 0.1 0.5)1   ⍝ everything\n",
    "        r\n",
    "    }\n",
    "    ⍝ Assesses capability to follow a long, narrow, curving multi-dimensional valley, testing\n",
    "    ⍝ the interplay between step length and direction adjustments controlled by the damping factor\n",
    "    t.Helical←{\n",
    "        T←{1|1+(12○⍺+0j1×⍵)÷○2} ⋄ M←{|⍺+0j1×⍵}\n",
    "        H←{\n",
    "            a b c←⍵ ⋄ m←a M b ⋄ y←(10×c-10×a T b)(10×m-1)c\n",
    "            y[((50×b)÷○+.×⍨a b)(-(50×a)÷○+.×⍨a b)10 ⋄ (10×a÷m)(10×b÷m)0 ⋄ 0 0 1]\n",
    "        }\n",
    "        r←1 0 0 CMP(R H #.LMA¯1 0 0).p                                ⍝ standard starting point\n",
    "        r,←1 0 0 CMP(R H #.LMA¯1.2 0.1 0.1).p                         ⍝ slightly perturbed\n",
    "        r,←1 0 0 CMP(R H #.LMA¯0.9 ¯0.05 ¯0.05).p                     ⍝ in other direction\n",
    "        r,←1 0 0 CMP(R H #.LMA 0.5 ¯0.5 0.5).p                        ⍝ qualitatively different\n",
    "        r,←1 0 0 CMP(R H #.LMA¯0.5 0.5 ¯0.5).p                        ⍝ another one\n",
    "        r,←1 0 0 CMP(R H #.LMA¯1 0 10).p                              ⍝ far off in 3rd dimension\n",
    "        r,←1 0 0 CMP(R H #.LMA¯1 0 ¯10).p                             ⍝ far off in 3rd dimension\n",
    "        r,←1 0 0 CMP(R H #.LMA 3 4 5).p                               ⍝ away in all components\n",
    "        r\n",
    "    }\n",
    "    ⍝ Verifies efficiency and correctness for a linear system. Converge should be very fast\n",
    "    ⍝ (1 or 2 iterations) with minimal damping, demonstrating Gauss-Newton like behavior\n",
    "    t.Linear←{\n",
    "        y←(A←?100 10⍴0)+.×x←⍳10 ⋄ s←R{(y-⍨A+.×⍵)A}#.LMA(?10⍴0)0 ⋄ (1=s.iter),(⍳10)CMP s.p\n",
    "    }\n",
    "    ⍝ numerical approximation of the Jacobian\n",
    "    t.LinearNum←{\n",
    "        y←(A←?100 5⍴0)+.×x←⍳5 ⋄ s←R{y-⍨A+.×⍵}#.LMA(?5⍴0)0 ⋄ (⍳5)CMP s.p\n",
    "    }\n",
    "    ⍝ Evaluates LMA's robustness and ability to converge to a solution when the Jacobian\n",
    "    ⍝ becomes singular (rank-deficient) at or near the optimum\n",
    "    t.Powell←{\n",
    "        P←{\n",
    "            a b c d←⍵ ⋄ r5 r10←5 10*÷2 ⋄ y←(a+10×b)(r5×c-d)(×⍨b-2×c)(r10××⍨a-d)\n",
    "            y[1 10 0 0 ⋄ 0 0 r5(-r5) ⋄ 0(2×b-2×c)(-2×b-2×c)0 ⋄ (2×r10×a-d)0 0(-2×r10×a-d)]\n",
    "        }\n",
    "        r←(4⍴0)CMP(R P #.LMA(3 ¯1 0 1)(tolc:1e¯30)).p                 ⍝ standard starting point\n",
    "        r,←(4⍴0)CMP(R P #.LMA(0 0 0 0)(tolc:1e¯30)).p                 ⍝ solution (convergence at zero step)\n",
    "        r,←(4⍴0)CMP(R P #.LMA(1 1 1 1)(tolc:1e¯30)).p                 ⍝ far from solution\n",
    "        r\n",
    "    }\n",
    "    ⍝ Tests LMA's performance in minimizing a classic non-linear function characterized by\n",
    "    ⍝ a deep, narrow, banana-shaped valley, requiring effective adaptation of search\n",
    "    ⍝ direction and step size\n",
    "    Rosenbrock←{p q←⍵ ⋄ ((10×q-×⍨p),1-p)[(-20×p)10 ⋄ ¯1 0]}\n",
    "    t.Rosenbrock←{\n",
    "        r←1 1 CMP(R #.Rosenbrock #.LMA 1.5 1.5).p                     ⍝ close to the solution\n",
    "        r,←1 1 CMP(R #.Rosenbrock #.LMA 2 1).p                        ⍝ not so close\n",
    "        r,←1 1 CMP(R #.Rosenbrock #.LMA 0 0).p                        ⍝ outside of parabollic valley\n",
    "        r,←1 1 CMP(R #.Rosenbrock #.LMA ¯1.2 1).p                     ⍝ further\n",
    "        r,←1 1 CMP(R #.Rosenbrock #.LMA ¯2 ¯2).p                      ⍝ far and wrongly pointed gradient\n",
    "        r,←1 1 CMP(R #.Rosenbrock #.LMA 2 2).p                        ⍝ far and wrongly pointed gradient\n",
    "        r\n",
    "    }\n",
    "    ⍝ numerical approximation of the Jacobian\n",
    "    t.RosenbrockNum←{\n",
    "        r←1 1 CMP(R⊃⍤#.Rosenbrock #.LMA 1.5 1.5).p                    ⍝ close to the solution\n",
    "        r,←1 1 CMP(R⊃⍤#.Rosenbrock #.LMA 2 1).p                       ⍝ not so close\n",
    "        r,←1 1 CMP(R⊃⍤#.Rosenbrock #.LMA 0 0).p                       ⍝ outside of parabollic valley\n",
    "        r,←1 1 CMP(R⊃⍤#.Rosenbrock #.LMA ¯1.2 1).p                    ⍝ further\n",
    "        r,←1 1 CMP(R⊃⍤#.Rosenbrock #.LMA ¯2 ¯2).p                     ⍝ far and wrongly pointed gradient\n",
    "        r,←1 1 CMP(R⊃⍤#.Rosenbrock #.LMA 2 2).p                       ⍝ far and wrongly pointed gradient\n",
    "        r\n",
    "    }\n",
    "    ⍝ Check that algorithm finishes on termination conditions\n",
    "    t.Terminate←{\n",
    "        r←(R #.Rosenbrock #.LMA(0 0)(toli:10)).(iter>toli)                      ⍝ number of iterations\n",
    "        r,←(R #.Rosenbrock #.LMA(0 0)(tolc:0)).(rel<tolr)                       ⍝ relative change\n",
    "        r,←(R #.Rosenbrock #.LMA(0 0)(tolc:0 ⋄ dmax:1e6)).((dnorm>1e6)∧rel=0)   ⍝ maximum damping\n",
    "        r\n",
    "    }\n",
    "    ⍺←1e¯6 ⋄ tol←⍺ ⋄ CMP←{tol>|⍺-⍵} ⋄ R←{⎕←(10↑''),⍵.((6 0⍕iter),(12 ¯5⍕cost),p) ⋄ ⍵}\n",
    "    (t⎕NS'tol' 'CMP' 'R'){⎕←⍵ ⋄ 0∊⍺⍎⍵,'⍬':('TEST FAILED: ',⍵)⎕SIGNAL 8 ⋄ _←0}¨(↓t.⎕NL 3)⊣⍣(⍵≡'*')⊆⍵\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccf7d8b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\">Beale        \n",
       "               6 3.4270E¯21  3 0.4999999999\n",
       "               9 1.4645E¯21  3 0.5\n",
       "              32 3.1182E¯17  2.99999998 0.4999999949\n",
       "               8 2.6016E¯19  3 0.5\n",
       "             846 2.2605E¯1   ¯15369.87905 1.000064478\n",
       "             844 2.2605E¯1   ¯15369.90862 1.000064477\n",
       "BealeNum     \n",
       "               6 3.4131E¯21  3 0.4999999999\n",
       "               9 1.4624E¯21  3 0.5\n",
       "              32 3.1173E¯17  2.99999998 0.4999999949\n",
       "               8 2.5785E¯19  3 0.5\n",
       "             816 2.2605E¯1   ¯15421.21242 1.000064263\n",
       "             824 2.2605E¯1   ¯15392.12945 1.000064384\n",
       "ExpDecFit    \n",
       "               1 0.0000E0    10 0.5 1\n",
       "               1 0.0000E0    10 0.5 1\n",
       "               1 0.0000E0    10 0.5 1\n",
       "               8 3.6086E¯1   9.963081164 0.5016881764 1.004084754\n",
       "               9 9.8500E0    9.962350702 0.5019804723 1.005512492\n",
       "            1001 3.0048E¯4   9.998625667 0.4999288685 1.000004261\n",
       "               9 8.5776E¯2   9.963068573 0.5016652526 1.00402812\n",
       "              10 9.7879E0    9.961998523 0.5022511401 1.006456899\n",
       "              13 5.2351E¯2   9.963090425 0.5017453135 1.004238847\n",
       "ExpDecJac    \n",
       "                 6.0285E¯12 \n",
       "                 3.1913E¯12 \n",
       "ExpDecLM     \n",
       "               5 9.6447E¯16  9.999999984 0.4999999965 1\n",
       "               5 9.6867E¯16  9.999999984 0.4999999965 1\n",
       "               5 1.9373E¯15  9.999999984 0.4999999965 1\n",
       "Helical      \n",
       "              10 5.9596E¯20  1 2.177303483E¯10 3.448140016E¯10\n",
       "              10 5.9231E¯20  1 2.170636163E¯10 3.437591012E¯10\n",
       "               8 2.3348E¯19  1 4.309357625E¯10 6.82517468E¯10\n",
       "              14 2.3416E¯20  1 1.364817538E¯10 2.161410116E¯10\n",
       "              12 7.8592E¯21  1 7.90691E¯11 1.252178359E¯10\n",
       "              24 1.3710E¯20  1 1.044305611E¯10 1.65382662E¯10\n",
       "              21 4.2751E¯15  1 5.82997047E¯8 9.234613263E¯8\n",
       "              10 1.8140E¯20  1 1.201260465E¯10 1.902399678E¯10\n",
       "Linear       \n",
       "               1 8.7699E¯25  1 2 3 4 5 6 7 8 9 10\n",
       "LinearNum    \n",
       "               1 5.6547E¯16  0.9999999986 1.999999996 3.000000002 3.999999994 5.000000008\n",
       "Powell       \n",
       "              57 9.9116E¯31  2.686746826E¯8 ¯2.686746826E¯9 1.720850692E¯8 1.720850692E¯8\n",
       "               1 0.0000E0    0 0 0 0\n",
       "              55 9.8879E¯31  ¯2.687933344E¯8 2.687933344E¯9 ¯1.719400136E¯8 ¯1.719400136E¯8\n",
       "Rosenbrock   \n",
       "               8 3.7366E¯15  1.000000086 1.00000017\n",
       "              12 3.9098E¯15  1.000000088 1.000000174\n",
       "              32 2.5636E¯15  0.999999929 0.999999857\n",
       "              28 2.4274E¯15  0.9999999309 0.9999998609\n",
       "              18 2.5797E¯15  0.9999999287 0.9999998566\n",
       "              14 4.0279E¯15  1.000000089 1.000000176\n",
       "RosenbrockNum\n",
       "               8 3.7370E¯15  1.000000086 1.00000017\n",
       "              12 3.9102E¯15  1.000000088 1.000000174\n",
       "              32 2.5633E¯15  0.999999929 0.999999857\n",
       "              28 2.4272E¯15  0.9999999309 0.9999998609\n",
       "              18 2.5795E¯15  0.9999999287 0.9999998566\n",
       "              14 4.0284E¯15  1.000000089 1.000000176\n",
       "Terminate    \n",
       "              11 5.0000E¯1   0 0\n",
       "              34 7.1244E¯30  1 1\n",
       "              13 5.0000E¯1   0 0\n",
       "</pre>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test'*'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f98f453",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc2f2e54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "] _←link.export -overwrite # APLSource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183bd502",
   "metadata": {},
   "source": [
    "## Tutorial\n",
    "\n",
    "This section provides practical examples to guide in the application of the Levenberg-Marquardt algorithm and the `LMA` and `LMA` operators to the solution of optiomization problems. The examples cover scenarios from simple equation solving to robust non-linear least squares fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b922bd2",
   "metadata": {},
   "source": [
    "### 1. Solving a simple linear system\n",
    "\n",
    "This example demonstrates the most basic use of `LMA` to solve a known linear system $A p = b$\n",
    "by minimizing $||A p - b||$. The algorithm should behave like Gauss-Newton for linear problems, converging very quickly to the analytical solution $p = A^{-1} b$.\n",
    "\n",
    "To begin, we define a random matrix `A` and random vector `b`. The solution parameters `p` can then be found using matrix division with `b⌹A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f15d6b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\">┌─────────────┬─────────┬──────────────┐\n",
       "│b            │A        │p             │\n",
       "├─────────────┼─────────┼──────────────┤\n",
       "│0.2650362441 │2 4 5 7 9│ 0.04223479208│\n",
       "│0.5414495595 │3 6 7 8 2│ 0.2006503854 │\n",
       "│0.01114182742│6 2 4 4 4│¯0.2872291459 │\n",
       "│0.6332729126 │8 8 5 1 1│ 0.1614620747 │\n",
       "│0.1881983241 │9 7 7 3 2│¯0.03512485279│\n",
       "└─────────────┴─────────┴──────────────┘\n",
       "</pre>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "⎕RL←1 ⋄ A←?5 5⍴9 ⋄ b←?5⍴0 ⋄ p←b⌹A\n",
    "'bAp'⍪⍉⍪⍪¨b A p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46f012a",
   "metadata": {},
   "source": [
    "Next, we define an evaluation function. This function must return the residuals and Jacobian for a set of solution parameters. The residual is $A p - b$, because the algorithm will minimize the sum of squares of the residual. Therefore, the Jacobian is the matrix $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6207da3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\">┌───────────────┬─────────┐\n",
       "│b-A+.×p        │A        │\n",
       "├───────────────┼─────────┤\n",
       "│1.110223025E¯16│2 4 5 7 9│\n",
       "│1.110223025E¯16│3 6 7 8 2│\n",
       "│1.110223025E¯16│6 2 4 4 4│\n",
       "│1.110223025E¯16│8 8 5 1 1│\n",
       "│2.220446049E¯16│9 7 7 3 2│\n",
       "└───────────────┴─────────┘\n",
       "</pre>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinEval←{A b←⍺ ⋄ (b-⍨A+.×⍵)A}     ⍝ residual and jacobian for solution parameters ⍵\n",
    "'b-A+.×p' 'A'⍪⍉⍪⍪¨A b LinEval p   ⍝ eg: at the solution point the residual should be zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a386b",
   "metadata": {},
   "source": [
    "To specify configuration options, we define a configuration namespace `cfg`. We must provide an evaluation function (the `LinEval` function that we just defined), and we will also set a very tight cost tolerance , since we know that an exact solution exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faae2b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg←(tolc:1e¯20)       ⍝ cost tolerance\n",
    "cfg.Eval←A b∘LinEval   ⍝ evaluation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3531859",
   "metadata": {},
   "source": [
    "Everything else we need to run the algorithm is an initial guess. We use a vector of ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10793cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\">┌──────────────┬──────────────┬────────────────┐\n",
       "│p             │r.p           │∆               │\n",
       "├──────────────┼──────────────┼────────────────┤\n",
       "│ 0.04223479208│ 0.04223479208│ 2.274291866E¯13│\n",
       "│ 0.2006503854 │ 0.2006503854 │ 2.296773882E¯13│\n",
       "│¯0.2872291459 │¯0.2872291459 │¯7.408518243E¯13│\n",
       "│ 0.1614620747 │ 0.1614620747 │ 4.187206137E¯13│\n",
       "│¯0.03512485279│¯0.03512485279│¯7.389922008E¯14│\n",
       "└──────────────┴──────────────┴────────────────┘\n",
       "</pre>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r←cfg LMA 5⍴1                    ⍝ run algorithm\n",
    "'p' 'r.p' '∆'⍪⍉⍪⍪¨p r.p(p-r.p)   ⍝ compare result and known solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031cc4a9",
   "metadata": {},
   "source": [
    "The parameters found are very close to the true parameters! We can also inspect other output variables. The sum of squared errors (the cost value) should be low, and we should have achieved convergence very quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a718a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\"> iter   7              \n",
       " cost   1.673636762E¯25\n",
       " rel    8.389067503E¯17\n",
       " dnorm  6.399999900E¯5 \n",
       "     p  0.04223479208 0.2006503854 ¯0.2872291459 0.1614620747 ¯0.03512485279 \n",
       "</pre>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RES←{⍺←'iter' 'cost' 'rel' 'dnorm' ⋄ ⍕⍉↑⍺(⍵⎕VGET↑⍺)} ⋄ P←{⍕(¯5↑'p')⍵.p}\n",
    "RES r ⋄ P r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cc92f2",
   "metadata": {},
   "source": [
    "We needed a few iterations to solve the problem. Let's use the `verbose` option to see the progress of the algorithm during each iteration. To pass an additional option without modifying our `cfg` namespace, we will use a namespace in the right argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "968af3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\">      0   1.5181E3      0.0000E0      1.0000E0      1.0000E0    1.0000E0    1.0000E0    1.0000E0    1.0000E0   \n",
       "      1   2.3662E¯2     9.7451E¯1     1.0000E0      2.3671E¯3   1.2489E¯1  ¯9.4886E¯2   6.3142E¯2  ¯1.7331E¯2  \n",
       "      2   2.1268E¯3     5.9610E¯1     2.0000E¯1     1.7103E¯2   1.7408E¯1  ¯2.0371E¯1   1.1462E¯1  ¯2.6999E¯2  \n",
       "      3   4.0359E¯5     9.1412E¯2     4.0000E¯2     3.8707E¯2   1.9708E¯1  ¯2.7573E¯1   1.5496E¯1  ¯3.3979E¯2  \n",
       "      4   3.8775E¯8     1.3337E¯3     8.0000E¯3     4.2125E¯2   2.0054E¯1  ¯2.8687E¯1   1.6126E¯1  ¯3.5089E¯2  \n",
       "      5   1.5669E¯12    1.2586E¯6     1.6000E¯3     4.2234E¯2   2.0065E¯1  ¯2.8723E¯1   1.6146E¯1  ¯3.5125E¯2  \n",
       "      6   2.5587E¯18    5.1269E¯11    3.2000E¯4     4.2235E¯2   2.0065E¯1  ¯2.8723E¯1   1.6146E¯1  ¯3.5125E¯2  \n",
       "      7   1.6736E¯25    8.3891E¯17    6.4000E¯5     4.2235E¯2   2.0065E¯1  ¯2.8723E¯1   1.6146E¯1  ¯3.5125E¯2  \n",
       " s: 1.6736E¯25   p: 4.2235E¯2   2.0065E¯1  ¯2.8723E¯1   1.6146E¯1  ¯3.5125E¯2\n",
       "</pre>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg LMA(5⍴1)(verbose:1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301582a5",
   "metadata": {},
   "source": [
    "The iteration number is in the first column; cost, relative change and normalized damping factor are in the second, third and fourth columns; and the accepted solution parameters follow.\n",
    "\n",
    "We observe that the cost is reduced during each iteration without problems. This indicates that we can reduce the damping factor. Actually, we can reduce it to the minimum using `0` as normalized damping factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2d32ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\">      0   1.5181E3      0.0000E0      0.0000E0      1.0000E0    1.0000E0    1.0000E0    1.0000E0    1.0000E0   \n",
       "      1   4.3916E¯25    9.9757E¯1     0.0000E0      4.2235E¯2   2.0065E¯1  ¯2.8723E¯1   1.6146E¯1  ¯3.5125E¯2  \n",
       " s: 4.3916E¯25   p: 4.2235E¯2   2.0065E¯1  ¯2.8723E¯1   1.6146E¯1  ¯3.5125E¯2\n",
       "</pre>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg LMA(5⍴1)0(verbose:1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44327f19",
   "metadata": {},
   "source": [
    "Convergence is achieved in a single iteration now!\n",
    "\n",
    "Notice the flexibility of the different ways in which options can be passed to `LMA`. Alternatively, we can also pass the evaluation function directly, avoiding having to predeclare a namespace. Additional options can be specified in the right argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bc400bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\">      0   1.5181E3      0.0000E0      0.0000E0      1.0000E0    1.0000E0    1.0000E0    1.0000E0    1.0000E0   \n",
       "      1   4.3916E¯25    9.9757E¯1     0.0000E0      4.2235E¯2   2.0065E¯1  ¯2.8723E¯1   1.6146E¯1  ¯3.5125E¯2  \n",
       " s: 4.3916E¯25   p: 4.2235E¯2   2.0065E¯1  ¯2.8723E¯1   1.6146E¯1  ¯3.5125E¯2\n",
       "</pre>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A b LinEval LMA(5⍴1)0(tolc:1e¯20 ⋄ verbose:1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5daf3d9",
   "metadata": {},
   "source": [
    "### 2. Fundamentals of non-linear least squares (NLLS) fitting\n",
    "\n",
    "In the next example, we will fit the parameters of a non-linear model. We will consider the exponentical decay function:\n",
    "\n",
    "$$y = C + A e^{-k x}$$\n",
    "\n",
    "Let's generate some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "346fe56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\">┌───┬───────────┬───────────┬───────────────┐\n",
       "│x  │y0         │y          │y-y0           │\n",
       "├───┼───────────┼───────────┼───────────────┤\n",
       "│0.1│2.060653066│2.065794637│ 0.005141570806│\n",
       "│0.2│2.036787944│2.039552083│ 0.002764138412│\n",
       "│0.3│2.022313016│2.024316539│ 0.002003522932│\n",
       "│0.4│2.013533528│2.009636489│¯0.003897039077│\n",
       "│0.5│2.0082085  │2.007880145│¯0.000328354821│\n",
       "│0.6│2.004978707│2.002851317│¯0.00212739019 │\n",
       "│0.7│2.003019738│2.007653027│ 0.004633288906│\n",
       "│0.8│2.001831564│1.997651869│¯0.004179695274│\n",
       "│0.9│2.0011109  │1.996961868│¯0.004149031743│\n",
       "│1  │2.000673795│1.998811638│¯0.001862156558│\n",
       "└───┴───────────┴───────────┴───────────────┘\n",
       "</pre>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "⎕RL←1\n",
    "ExpDec←{A k C←⍺ ⋄ C+A×*-k×⍵}                ⍝ exponential decay\n",
    "Noise←{⍵×1+⍺×0.5-?≠⍨⍵}                      ⍝ add random noise of ±⍺ to ⍵\n",
    "y0←(p←0.1 5 2)ExpDec x←(⍳100)÷10            ⍝ true data (100 data points)\n",
    "y←5e¯3 Noise y0                             ⍝ add some random noise (±0.5%)\n",
    "'x' 'y0' 'y' 'y-y0'⍪⍉⍪(10↑⍪)¨x y0 y(y-y0)   ⍝ first 10 data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca2cd26",
   "metadata": {},
   "source": [
    "The `ExpDec` function evaluates the exponential decay function in the selected points for the parameters given. We use it to generate true values `y0` using known parameters `p`. Then, using the function `Noise`, we apply a random variation to all the points (of a maximum of 0.5%), to get the more realistic data in `y`. This imperfect data is a better representation of the kind of data usually found in fitting problems.\n",
    "\n",
    "To fit the parameters using `LMA`, we need an evaluation function. As in previous example, we are going to define a function that returns the residual and the Jacobian. The Jacobian for the exponentical decay function defined above is given by the matrix:\n",
    "\n",
    "$$J = \\begin{bmatrix}\n",
    "-e^{-kx_1} & A x_1 e^{-kx_1} & -1 \\\\\n",
    "-e^{-kx_2} & A x_2 e^{-kx_2} & -1 \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "-e^{-kx_m} & A x_m e^{-kx_m} & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Our evaluation function therefore takes the form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e669adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ExpDecEval←{x y←⍺ ⋄ A k C←⍵ ⋄ xe←A×x×e←*-k×x ⋄ (y-⍵ ExpDec x)(⍉(-e)⍪xe⍪⍉⍪-=⍨e)}   ⍝ residual and jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ad409",
   "metadata": {},
   "source": [
    "With the `Eval` function defined, we can now run the LMA solver. We will use again a vector of ones as initial guess. Let's first fit the parameters using the raw output of the `ExpDec` function store in `y0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4971d21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\"> iter   6              \n",
       " cost   7.395448698E¯15\n",
       " rel    2.423744115E¯7 \n",
       " dnorm  3.199999990E¯4 \n",
       "     p  0.09999997999 4.999990499 2 \n",
       "</pre>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RP←{⎕←RES ⍵ ⋄ ⎕←P ⍵}\n",
    "RP x y0 ExpDecEval LMA 3⍴1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b66e826",
   "metadata": {},
   "source": [
    "The algorithm converges in very few iterations, and the solution parameters are very close to the true parameters (`0.1 5 2`) used to generate `y0`. The cost is extremely low, as expected for a fit to clean data.\n",
    "\n",
    "Now we attempt to find the parameters using the noisy data in `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f2813ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\"> iter   9              \n",
       " cost   4.227554471E¯4 \n",
       " rel    1.367768059E¯16\n",
       " dnorm  2.559999000E¯6 \n",
       "     p  0.113926342 5.363226638 1.999684665 \n",
       "</pre>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RP x y ExpDecEval LMA 3⍴1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ddbcc5",
   "metadata": {},
   "source": [
    "When fitting the noisy data `y`, the algorithm still converges. The solution parameters are reasonably close to the true values, but the final cost is higher, reflecting the noise. The algorithm stopped because the relative change (`rel`) fell below `tolr`, but the cost could not be reduced below tolerance. We can try to refine the solution reducing the `tolr` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f7f9e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\"> iter   11              \n",
       " cost    4.227554471E¯4 \n",
       " rel     1.458118613E¯23\n",
       " dnorm   1.023990000E¯7 \n",
       "     p  0.113926342 5.363226637 1.999684665 \n",
       "</pre>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RP x y ExpDecEval LMA(3⍴1)(tolr:1e¯20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cfed91",
   "metadata": {},
   "source": [
    "Even with a much stricter `tolr`, the solution parameters and cost do not change significantly, indicating that we have found a good local minimum for this noisy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79dd7bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\">      0   4.4650E¯4     0.0000E0      1.0000E0      1.0000E¯1   5.0000E0    2.0000E0   \n",
       "0.0004464956831\n",
       "</pre>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x y ExpDecEval LMA p(toli:0 ⋄ verbose:1)).cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4615d",
   "metadata": {},
   "source": [
    "By running LMA with the true parameters `p` and `toli:0` (0 iterations, so just one evaluation), the resulting cost shows the inherent sum of losses for a perfect model with noisy data. This is the theoretical minimum cost `LMA` could achieve for this noisy data with these true parameters. The algorithm actually yielded a slightly lower cost, indicating that we are *overfitting* (adjusting the parameters to the noise instead of real data).\n",
    "\n",
    "#### Numerical estimation of Jacobian matrix\n",
    "\n",
    "What if we do not know what is the Jacobian and only have a residual function available? In this case, the algorithm can still find a solution, using a numerical estimation of the Jacobian.\n",
    "\n",
    "Let's consider the exponential decay function again. But, this time, we will not use the `ExpDecEval` function. Instead, we define an `ExpDecRes` function that only returns the residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55dc396b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\"> iter   9              \n",
       " cost   4.227554471E¯4 \n",
       " rel    2.308147563E¯15\n",
       " dnorm  2.559999000E¯6 \n",
       "     p  0.1139263484 5.363227017 1.999684665 \n",
       "</pre>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExpDecRes←{x y←⍺ ⋄ y-⍵ ExpDec x}\n",
    "RP x y ExpDecRes LMA(3⍴1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ffac3c",
   "metadata": {},
   "source": [
    "Although it took a few more iterations, the results are very similar to those obtained with the analytical Jacobian. `LMA` automatically detected that `ExpDecRes` only returned residuals and used the `Jacobian` operator (with the default `pert` value `⎕CT*÷2`) to approximate the Jacobian.\n",
    "\n",
    "Modifying the configuration parameter `pert`, it is possible to adjust the applied perturbation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee40bdfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\"> iter   9              \n",
       " cost   4.227554471E¯4 \n",
       " rel    2.308147563E¯15\n",
       " dnorm  2.559999000E¯6 \n",
       "     p  0.1139263484 5.363227017 1.999684665 \n",
       "</pre>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\"> iter   23              \n",
       " cost    4.227573288E¯4 \n",
       " rel     4.524970054E¯15\n",
       " dnorm   2.500000000E1  \n",
       "     p  0.1137824911 5.354739109 1.999683407 \n",
       "</pre>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\"> iter   31              \n",
       " cost    1.710624289E¯3 \n",
       " rel     5.578222705E¯16\n",
       " dnorm   9.765625010E6  \n",
       "     p  0.03468243617 1.138692831 1.998389309 \n",
       "</pre>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RP x y ExpDecRes LMA(3⍴1)(pert:⎕CT*÷2)   ⍝ default perturbation\n",
    "RP x y ExpDecRes LMA(3⍴1)(pert:1e0)      ⍝ large perturbation, worse prediction makes convergence slower\n",
    "RP x y ExpDecRes LMA(3⍴1)(pert:1e¯15)    ⍝ perturbation too small, inacurate result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22a45a",
   "metadata": {},
   "source": [
    "Very large perturbations can degrade the Jacobian approximation, resulting in slow convergence, while extremely small ones (like `1E-15`) can lead to numerical inaccuracies and poor solutions due to precision limits in finite differences. The default perturbation provides a good balance in this case.\n",
    "\n",
    "For an accurate result, the Jacobian calculated numerically should be close to the analytical Jacobian. However, the quality of the approximation can vary depending on the actual solution parameters being used, as we can see comparing the Jacobian calculated numerically with the analytical Jacobian defined earlier. We calculate a relative error for the initial and final solution parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80d7834e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\">1.747428903E¯16\n",
       "</pre>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\">5.794393457E¯18\n",
       "</pre>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(⊃⌽x y ExpDecEval 3⍴1)(-÷⍥(+.×⍨,)⊣)x y∘ExpDecRes Jacobian 3⍴1  ⍝ initial relative error\n",
    "(⊃⌽x y ExpDecEval p)(-÷⍥(+.×⍨,)⊣)x y∘ExpDecRes Jacobian p      ⍝ final relative error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9984d898",
   "metadata": {},
   "source": [
    "The relative error between the analytical Jacobian and our numerical approximation (using the default perturbation) is very small, especially for the final solution parameters, confirming the reliability of the numerical estimation.\n",
    "\n",
    "#### Callback function\n",
    "\n",
    "We could be interested in monitoring some variable during the optimization process. For example, in the last example, we might want to know how the numerical approximation of the Jacobian is progressing. This can be achieved using the callback function. The `CallBack` function, defined in the configuration namespace, is run every iteration with a namespace argument like the one returned by `LMA` as result, allowing the user to perform a detailed diagnosis.\n",
    "\n",
    "Here, we define a callback function that displays the relative error with respect to the analytical Jacobian every iteration, together with the cost, relative error, and last accepted guess for the solution parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bed7983b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\"> errJ        cost        rel         A           k           C          \n",
       " 5.1178E¯2   4.2757E1    0.0000E0    1.0000E0    1.0000E0    1.0000E0   \n",
       " 2.6863E¯2   9.5101E¯3   6.1341E¯1   6.6774E¯2   9.9423E¯1   1.9845E0   \n",
       " 1.0724E¯2   1.5729E¯3   7.7278E¯2   4.9287E¯2   1.6111E0    2.0007E0   \n",
       " 1.4052E¯3   6.8826E¯4   3.4073E¯1   6.8879E¯2   3.1107E0    2.0002E0   \n",
       " 4.2633E¯6   4.4088E¯4   2.2285E¯1   9.9420E¯2   4.8565E0    1.9998E0   \n",
       " 2.4487E¯5   4.2288E¯4   9.6043E¯3   1.1335E¯1   5.3712E0    1.9997E0   \n",
       " 2.3480E¯5   4.2276E¯4   2.0250E¯6   1.1392E¯1   5.3630E0    1.9997E0   \n",
       " 2.3505E¯5   4.2276E¯4   1.2926E¯9   1.1393E¯1   5.3632E0    1.9997E0   \n",
       " s: 4.2276E¯4    p: 1.1393E¯1   5.3632E0    1.9997E0\n",
       "</pre>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg←(tolr:1e¯6) ⋄ cfg.Eval←ExpDecRes ⋄ cfg.ExpDec←ExpDec\n",
    "cfg.CallBack←{⎕←12 ¯5⍕⍵.(cost rel,p),⍨(⊃⌽#.(x y ExpDecEval p))(-÷⍥(+.×⍨,)⊣)#.(x 0∘ExpDecRes Jacobian)⍵.p}\n",
    "⎕←∊⍕10↑¨'errJ' 'cost' 'rel' 'A' 'k' 'C' ⋄ x y(cfg LMA)3⍴1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bf5e64",
   "metadata": {},
   "source": [
    "### 3. Fitting of a physical model with experimental data\n",
    "\n",
    "At continuation, we are going to tackle a problem closer to a real life example. We will see how to fit the parameters of a Gaussian function:\n",
    "\n",
    "$$f(x) = c + a \\cdot \\exp\\left(-\\frac{(x-m)^2}{2s^2}\\right)$$\n",
    "\n",
    "Gaussian peaks are arguably one of the most frequently encountered shapes in scientific and engineering data (spectroscopy, chromatography, signal processing, statistical distributions, etc.). Its symmetric, peak-like structure behaves differently from the monotonic exponential decay function in previous example.\n",
    "\n",
    "For the fitting, we are going to produce virtual experimental data. First, we generate clean output data using a predefined set of parameters. Next, some small noise is added to the data, using the same `Noise` function from previous example. Finally, we create a few outliers applying a much larger perturbation, again using the `Noise` function, but with a larger value as left argument and applying it only to a few random points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c74ca03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\">┌────┬───────────┬────────────────┬───────────┬──────────────┬───────────┐\n",
       "│x   │y0         │∆small          │y1         │∆large        │y          │\n",
       "├────┼───────────┼────────────────┼───────────┼──────────────┼───────────┤\n",
       "│0   │3.87419801 │¯0.0249511715   │3.970863789│ 0            │3.970863789│\n",
       "│0.25│4.212887812│¯0.01357106625  │4.270061192│¯0.2613420563 │5.386007764│\n",
       "│0.5 │4.512484513│¯0.009907086175 │4.557190086│ 0            │4.557190086│\n",
       "│0.75│4.760512462│ 0.01935422987  │4.668376409│¯0.4749492285 │6.885618183│\n",
       "│1   │4.946166172│ 0.001635063396 │4.938078877│ 0            │4.938078877│\n",
       "│1.25│5.061089691│ 0.01061053757  │5.007388809│ 0            │5.007388809│\n",
       "│1.5 │5.1        │¯0.02313151897  │5.217970747│ 0            │5.217970747│\n",
       "│1.75│5.061089691│ 0.02087935543  │4.955417401│ 0            │4.955417401│\n",
       "│2   │4.946166172│ 0.02073364222  │4.843614133│ 0            │4.843614133│\n",
       "│2.25│4.760512462│ 0.009307647067 │4.716203292│ 0            │4.716203292│\n",
       "│2.5 │4.512484513│¯0.01474302463  │4.579012183│¯0.02582317868│4.697256833│\n",
       "│2.75│4.212887812│¯0.01809174955  │4.289106323│ 0            │4.289106323│\n",
       "│3   │3.87419801 │¯0.01595143831  │3.93599704 │ 0            │3.93599704 │\n",
       "│3.25│3.509703756│ 0.0009355548021│3.506420236│ 0            │3.506420236│\n",
       "│3.5 │3.132653299│¯0.006445134993 │3.152843672│ 0            │3.152843672│\n",
       "│3.75│2.755479955│ 0.006737319096 │2.736915407│ 0.2338351955 │2.096928258│\n",
       "│4   │2.389166809│ 0.007569904963 │2.371081043│ 0            │2.371081043│\n",
       "│4.25│2.042790638│¯0.01855836257  │2.080701487│ 0            │2.080701487│\n",
       "│4.5 │1.723262337│¯0.004494514645 │1.731007565│ 0            │1.731007565│\n",
       "│4.75│1.435259176│ 0.01540599528  │1.41314758 │¯0.1043841607 │1.560657804│\n",
       "└────┴───────────┴────────────────┴───────────┴──────────────┴───────────┘\n",
       "</pre>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "⎕RL←1 ⋄ X←{m s←⍺ ⋄ *-(×⍨⍵-m)÷(2××⍨s)}\n",
    "Gauss←{a s m c←⍺ ⋄ c+a×m s X ⍵}                                ⍝ gauss function\n",
    "y0←(p←5 2 1.5 0.1)Gauss x←0.25×(⍳20)-1                         ⍝ true data\n",
    "y←(rl←1 Noise@(5?≢x)=⍨x)×y1←(rs←5e¯2 Noise=⍨x)×y0              ⍝ random noise (±5%) and 5 outliers (±100%)\n",
    "'x' 'y0' '∆small' 'y1' '∆large' 'y'⍪⍉⍪⍪¨x y0(1-rs)y1(1-rl)y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7293ccd",
   "metadata": {},
   "source": [
    "We will fit the parameters using a numerical approximation of the Jacobian (we will also use the analytical one later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10c35d74",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\"> iter   16              \n",
       " cost    3.260096722E¯2 \n",
       " rel     4.818105743E¯20\n",
       " dnorm   1.279999900E¯5 \n",
       "     p  5.360498897 2.151620676 1.483878994 ¯0.2896122654 \n",
       "</pre>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\"> iter   15              \n",
       " cost    2.235416692E0  \n",
       " rel     3.306121866E¯16\n",
       " dnorm   2.559999000E¯6 \n",
       "     p  4.956807586 2.014555465 1.202540192 0.4350967403 \n",
       "</pre>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GaussRes←{x y←⍺ ⋄ y-⍵ Gauss x}\n",
    "RP x y1 GaussRes LMA(4⍴1)(verbose:0)\n",
    "RP x y GaussRes LMA(4⍴1)(verbose:0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042ba5bb",
   "metadata": {},
   "source": [
    "The first run on the data with small noise (`y0`) converges well, with the solution parameters reasonably close to the true values `5 2 1.5 0.1`. However, the second run on the data with significant outliers (`y`) shows that the L2 loss function is heavily influenced by these outliers, and the resulting parameters are poor, with a high final cost.\n",
    "\n",
    "#### Weighted least-squares fitting\n",
    "\n",
    "If we know beforehand what is the quality of each of our datapoints--for example, because the experimental device that we use give us data about the quality of a measurement--we can use this information to guide the fitting process. Assigning a different weight to each of the data points, we can perform what is called a *weighted least-squares* (WLS) fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0589eb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\"> iter   16              \n",
       " cost    2.994435871E¯2 \n",
       " rel     1.743218715E¯16\n",
       " dnorm   1.279999900E¯5 \n",
       "     p  5.353038299 2.149186423 1.48366159 ¯0.2820439403 \n",
       "</pre>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\"> iter   15              \n",
       " cost    1.006671035E0  \n",
       " rel     8.914381500E¯18\n",
       " dnorm   2.559999000E¯6 \n",
       "     p  5.058256596 2.079191318 1.327590531 0.1594953187 \n",
       "</pre>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RP x y1 GaussRes LMA(4⍴1)(scale:s1←÷1+|y1-y0)\n",
    "RP x y GaussRes LMA(4⍴1)(scale:s←÷1+|y-y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e77d9a",
   "metadata": {},
   "source": [
    "By applying weights (here, inversely proportional to the deviation from the true data `y0`), the WLS fit gives more importance to data points believed to be more reliable. For the `y1` data, the parameters are similar to the unweighted L2 fit. However, for the `y` data with outliers, WLS can improve the fit if the weights correctly down-weight the outlier points.\n",
    "\n",
    "#### Robust loss functions\n",
    "\n",
    "Data about the quality of a measurement is not always available. It is often the case that experimental data present noise and outliers, but we do not know in advance how to weight our data points to get an accurate result. In this case, the use of *robust loss functions* can help to find a better solution.\n",
    "\n",
    "We are going to repeat the fitting of the Gaussian function parameters using different robust functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08daee5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\"> s: 3.2601E¯2    p: 5.3605E0    2.1516E0    1.4839E0   ¯2.8961E¯1\n",
       "</pre>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\"> s: 2.9944E¯2    p: 5.3530E0    2.1492E0    1.4837E0   ¯2.8204E¯1\n",
       "</pre>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\"> s: 6.5202E¯2    p: 5.3605E0    2.1516E0    1.4839E0   ¯2.8961E¯1\n",
       "</pre>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\"> s: 3.2551E¯2    p: 5.3609E0    2.1519E0    1.4838E0   ¯2.9016E¯1\n",
       "</pre>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\"> s: 3.2461E¯2    p: 5.3617E0    2.1523E0    1.4838E0   ¯2.9113E¯1\n",
       "</pre>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\"> s: 4.5500E1     p: 1.0000E0    1.0000E0    1.0000E0    1.0000E0\n",
       "</pre>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\"> s: 3.2569E¯2    p: 5.3608E0    2.1518E0    1.4839E0   ¯2.8996E¯1\n",
       "</pre>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\"> s: 3.0204E¯2    p: 5.3659E0    2.1555E0    1.4828E0   ¯2.9694E¯1\n",
       "</pre>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\"> s: 3.2596E¯2    p: 5.3606E0   ¯2.1517E0    1.4839E0   ¯2.8976E¯1\n",
       "</pre>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "⊢r←x y1 GaussRes LMA(4⍴1)\n",
    "⊢r,←x y1 GaussRes LMA(4⍴1)(scale:s1)        ⍝ wls\n",
    "⊢r,←x y1 GaussRes LMA(4⍴1)(loss:'Huber')\n",
    "⊢r,←x y1 GaussRes LMA(4⍴1)(loss:'Cauchy')\n",
    "⊢r,←x y1 GaussRes LMA(4⍴1)(loss:'SoftL1')\n",
    "⊢r,←x y1 GaussRes LMA(4⍴1)(loss:'Tukey')\n",
    "⊢r,←x y1 GaussRes LMA(4⍴1)(loss:'Welsh')\n",
    "⊢r,←x y1 GaussRes LMA(4⍴1)(loss:'Fair')\n",
    "⊢r,←x y1 GaussRes LMA(4⍴1)(loss:'Arctan')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820fa2d9",
   "metadata": {},
   "source": [
    "With the exception of Tukey's loss function, all of them converge with a relatively low cost. Tukey's function usually requires a careful adjustment of the scaling factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "682d5dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\"> s: 2.3200E¯2    p: 5.0030E0    1.9893E0    1.4905E0    1.0167E¯1\n",
       "</pre>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "⊢r[5]←x y1 GaussRes LMA(4⍴1)(loss:'Tukey' ⋄ scale:0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aef1c5",
   "metadata": {},
   "source": [
    "To compare all loss functions, we can calculate the sum of squared residuals corresponding to the solution parameters obtained using each of the loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c589aafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\">┌────────────┬────────────┐\n",
       "│Robust      │L2          │\n",
       "├────────────┼────────────┤\n",
       "│ 3.2601E¯2  │ 3.2601E¯2  │\n",
       "│ 2.9944E¯2  │ 3.2604E¯2  │\n",
       "│ 6.5202E¯2  │ 3.2601E¯2  │\n",
       "│ 3.2551E¯2  │ 3.2601E¯2  │\n",
       "│ 2.3200E¯2  │ 4.6172E¯2  │\n",
       "│ 4.5500E1   │ 6.5772E1   │\n",
       "│ 3.2569E¯2  │ 3.2601E¯2  │\n",
       "│ 3.0204E¯2  │ 3.2617E¯2  │\n",
       "│ 3.2596E¯2  │ 3.2601E¯2  │\n",
       "└────────────┴────────────┘\n",
       "</pre>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl2←(x y1(GaussRes LMA)(toli:0),⍨⊂)¨r.p\n",
    "'Robust' 'L2'⍪⍉⍪(12 ¯5⍕⍪)¨r.cost rl2.cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118c442e",
   "metadata": {},
   "source": [
    "The `Robust` column shows the sum of the respective loss values, which are not directly comparable across different loss types. However, in the `L2` column, we see that when fitting the data with only small noise, most robust loss functions yield a final sum of squared residuals very similar to that obtained by the direct L2 fit. This demonstrates that robust loss functions do not significantly penalize performance on clean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a3493f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\">┌────────────┬────────────┐\n",
       "│Robust      │L2          │\n",
       "├────────────┼────────────┤\n",
       "│ 2.2354E0   │ 2.2354E0   │\n",
       "│ 1.0067E0   │ 2.4495E0   │\n",
       "│ 3.2862E0   │ 2.2842E0   │\n",
       "│ 1.7824E0   │ 2.2950E0   │\n",
       "│ 1.5281E0   │ 2.3450E0   │\n",
       "│ 3.7777E1   │ 1.2132E1   │\n",
       "│ 1.8620E0   │ 2.2868E0   │\n",
       "│ 1.1538E0   │ 2.3600E0   │\n",
       "│ 9.0551E¯1  │ 2.8876E0   │\n",
       "└────────────┴────────────┘\n",
       "</pre>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r←x y GaussRes LMA(4⍴1)\n",
    "r,←x y GaussRes LMA(4⍴1)(scale:s)         ⍝ wls\n",
    "r,←x y GaussRes LMA(4⍴1)(loss:'Huber')\n",
    "r,←x y GaussRes LMA(4⍴1)(loss:'Cauchy')\n",
    "r,←x y GaussRes LMA(4⍴1)(loss:'SoftL1')\n",
    "r,←x y GaussRes LMA(4⍴1)(loss:'Tukey')\n",
    "r,←x y GaussRes LMA(4⍴1)(loss:'Welsh')\n",
    "r,←x y GaussRes LMA(4⍴1)(loss:'Fair')\n",
    "r,←x y GaussRes LMA(4⍴1)(loss:'Arctan')\n",
    "rl2←(x y(GaussRes LMA)(toli:0),⍨⊂)¨r.p\n",
    "'Robust' 'L2'⍪⍉⍪(12 ¯5⍕⍪)¨r.cost rl2.cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13873ba4",
   "metadata": {},
   "source": [
    "When fitting the data containing significant outliers, the benefit of robust loss functions becomes clear. While the L2 fit is heavily skewed (high L2 cost), functions like Huber, Cauchy, SoftL1, Fair, and Arctan achieve a much lower sum of squared residuals by down-weighting the outliers. Their own cost is also minimized effectively. Tukey and Welsh, being redescending, can be very effective but may require careful tuning of their scale parameter, or more iterations, to converge optimally, especially if the initial scale (auto-estimated or default) is not ideal for the outlier distribution.\n",
    "\n",
    "#### `LM` operator\n",
    "\n",
    "The `LMA` operator provides a convenient interface for the user. However, the core engine can also be directly accessed using the `LM` operator (which `LMA` uses internally).\n",
    "\n",
    "Although the interface for `LM` is more \"arcane\", it offers the same flexibility as `LMA`. The evaluation function must be the left operand. The callback function (which in this case takes a vector instead of namespace) is specified as the right operand. All configuration parameters need to be passed as left argument, and the right argument must be a two elements vector with the initial parameters and normalized damping factor.\n",
    "\n",
    "The evaluation function may return either only the residual, the residual and the Jacobian, or also loss and weight values if it is desired to use a non-standard loss function. Let's see these three cases for the Gaussian function. We will need the analytical Jacobian given by:\n",
    "\n",
    "$$\n",
    "J_i = \\begin{bmatrix}\n",
    "-e^{-\\frac{(x_i-m)^2}{2s^2}} & \n",
    "-a \\cdot e^{-\\frac{(x_i-m)^2}{2s^2}} \\cdot \\frac{(x_i-m)^2}{s^3} & \n",
    "-a \\cdot e^{-\\frac{(x_i-m)^2}{2s^2}} \\cdot \\frac{(x_i-m)}{s^2} & \n",
    "-1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56b9269c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\">1.014351005E¯15\n",
       "</pre>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GaussJac←{a s m c←⍺ ⋄ x2←(x1←(xe←-m s X ⍵)×a×(⍵-m)÷×⍨s)×(⍵-m)÷s ⋄ ⍉↑(xe)x2(x1)(-=⍨xe)}\n",
    "(p GaussJac x)(-÷⍥(+.×⍨,)⊣)x y∘GaussRes Jacobian p   ⍝ check error with respect to numerical estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3a102c",
   "metadata": {},
   "source": [
    "We can now define three different evaluation functions (notice that they must be monadic functions, so we bind the left argument with `∘`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59b2c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "GaussEval1←x y∘GaussRes                         ⍝ residual only\n",
    "GaussEval2←x y∘{(⍺ GaussRes ⍵)(⍵ GaussJac⊃⍺)}   ⍝ residual and jacobian\n",
    "GaussEval3←(⊢,Loss.(fair∘Fair)⍤⊃)GaussEval2     ⍝ residual, jacobian, and fair loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2de9484",
   "metadata": {},
   "source": [
    "Finally, we run the algorithm using the default configuration parameters of `LMA`, and a `CallBack` function with a functionality similar to `verbose:1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "917eea1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\">  0 77.215082377   0.000000000   1.000000000   1.000000000 1.000000000 1.000000000 1.000000000\n",
       "  1 11.987065609   0.844757459   1.000000000   3.162978849 2.727267330 1.272949344 2.030246450\n",
       "  2 11.987065609   0.228400446   0.200000000   3.162978849 2.727267330 1.272949344 2.030246450\n",
       "  3 11.987065609   0.208087108   1.000000000   3.162978849 2.727267330 1.272949344 2.030246450\n",
       "  4 11.987065609   0.151019983   5.000000000   3.162978849 2.727267330 1.272949344 2.030246450\n",
       "  5  6.247480602   0.088946997  25.000000000   3.239695395 1.410734496 0.712543741 1.935310079\n",
       "  6  2.863887822   0.026737963   5.000000000   3.550801485 1.782248042 1.097438736 1.681121690\n",
       "  7  2.292328953   0.046311013   1.000000000   4.309319875 1.760002360 1.236176002 1.113172560\n",
       "  8  2.239637714   0.020148103   0.200000000   4.768992457 1.959750725 1.202078465 0.623314833\n",
       "  9  2.235452415   0.001868739   0.040000000   4.943812834 2.011998201 1.201744836 0.448049541\n",
       " 10  2.235416699   0.000011297   0.008000000   4.956738360 2.014570638 1.202505523 0.435159628\n",
       " 11  2.235416692   0.000000000   0.001600000   4.956793802 2.014549004 1.202541183 0.435111833\n",
       " 12  2.235416692   0.000000000   0.000320000   4.956809012 2.014556181 1.202540032 0.435095170\n",
       " 13  2.235416692   0.000000000   0.000064000   4.956807512 2.014555420 1.202540206 0.435096824\n",
       " 14  2.235416692   0.000000000   0.000012800   4.956807652 2.014555491 1.202540190 0.435096670\n",
       " 15  2.235416692   0.000000000   0.000002560   4.956807586 2.014555465 1.202540192 0.435096740\n",
       "┌──┬───────────┬───────────────┬──────────────┬────────────────────────────────────────────────┐\n",
       "│15│2.235416692│3.306121866E¯16│0.000002559999│4.956807586 2.014555465 1.202540192 0.4350967403│\n",
       "└──┴───────────┴───────────────┴──────────────┴────────────────────────────────────────────────┘\n",
       "</pre>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\">  0 77.215082377   0.000000000   1.000000000   1.000000000 1.000000000 1.000000000 1.000000000\n",
       "  1 11.987064561   0.844757472   1.000000000   3.162978743 2.727267116 1.272949392 2.030246543\n",
       "  2 11.987064561   0.228400321   0.200000000   3.162978743 2.727267116 1.272949392 2.030246543\n",
       "  3 11.987064561   0.208087028   1.000000000   3.162978743 2.727267116 1.272949392 2.030246543\n",
       "  4 11.987064561   0.151019570   5.000000000   3.162978743 2.727267116 1.272949392 2.030246543\n",
       "  5  6.247478919   0.088946973  25.000000000   3.239695322 1.410734519 0.712543835 1.935310179\n",
       "  6  2.863887746   0.026737963   5.000000000   3.550801494 1.782247989 1.097438792 1.681121707\n",
       "  7  2.292328945   0.046311014   1.000000000   4.309319889 1.760002400 1.236175975 1.113172536\n",
       "  8  2.239637713   0.020148101   0.200000000   4.768992459 1.959750724 1.202078460 0.623314832\n",
       "  9  2.235452415   0.001868739   0.040000000   4.943812783 2.011998181 1.201744836 0.448049593\n",
       " 10  2.235416699   0.000011297   0.008000000   4.956738374 2.014570647 1.202505520 0.435159611\n",
       " 11  2.235416692   0.000000000   0.001600000   4.956793822 2.014549012 1.202541182 0.435111813\n",
       " 12  2.235416692   0.000000000   0.000320000   4.956809043 2.014556195 1.202540029 0.435095137\n",
       " 13  2.235416692   0.000000000   0.000064000   4.956807411 2.014555378 1.202540211 0.435096932\n",
       " 14  2.235416692   0.000000000   0.000012800   4.956807614 2.014555479 1.202540189 0.435096709\n",
       "┌──┬───────────┬───────────────┬──────────────┬────────────────────────────────────────────────┐\n",
       "│14│2.235416692│1.191963569E¯15│0.000012799999│4.956807614 2.014555479 1.202540189 0.4350967092│\n",
       "└──┴───────────┴───────────────┴──────────────┴────────────────────────────────────────────────┘\n",
       "</pre>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<pre class=\"language-APL\">  0 26.896757586   0.000000000   1.000000000   1.000000000 1.000000000 1.000000000 1.000000000\n",
       "  1  6.334011983   0.764506485   1.000000000   3.145538156 2.936213343 1.416108353 1.886657074\n",
       "  2  6.334011983   0.291511045   0.200000000   3.145538156 2.936213343 1.416108353 1.886657074\n",
       "  3  6.334011983   0.282114932   1.000000000   3.145538156 2.936213343 1.416108353 1.886657074\n",
       "  4  6.334011983   0.239942452   5.000000000   3.145538156 2.936213343 1.416108353 1.886657074\n",
       "  5  5.060116840   0.141468387  25.000000000   3.212505577 1.203909890 0.783109994 1.862235960\n",
       "  6  2.035570084   0.023462366   5.000000000   3.271190190 1.754097324 1.024785457 1.776564669\n",
       "  7  1.446586300   0.081402255   1.000000000   4.188654453 1.870188441 1.297924884 1.045737834\n",
       "  8  1.329907437   0.036226904   0.200000000   4.847286551 2.038210604 1.279424701 0.412826371\n",
       "  9  1.322673316   0.004472512   0.040000000   5.097415256 2.116483396 1.267973422 0.161971001\n",
       " 10  1.322431553   0.000174928   0.008000000   5.149268470 2.133744563 1.263979287 0.110863044\n",
       " 11  1.322422191   0.000001180   0.001600000   5.153698088 2.135566420 1.262764660 0.107102549\n",
       " 12  1.322421261   0.000000033   0.000320000   5.154417472 2.135970881 1.262370709 0.106612872\n",
       " 13  1.322421162   0.000000003   0.000064000   5.154646793 2.136106671 1.262241305 0.106456716\n",
       " 14  1.322421151   0.000000000   0.000012800   5.154722464 2.136151784 1.262198902 0.106404759\n",
       " 15  1.322421150   0.000000000   0.000002560   5.154747289 2.136166606 1.262185023 0.106387673\n",
       " 16  1.322421150   0.000000000   0.000000512   5.154755416 2.136171460 1.262180482 0.106382075\n",
       " 17  1.322421150   0.000000000   0.000000102   5.154758076 2.136173048 1.262178996 0.106380243\n",
       " 18  1.322421150   0.000000000   0.000000020   5.154758946 2.136173568 1.262178510 0.106379644\n",
       " 19  1.322421150   0.000000000   0.000000004   5.154759231 2.136173738 1.262178351 0.106379448\n",
       "┌──┬──────────┬───────────────┬────────┬────────────────────────────────────────────────┐\n",
       "│19│1.32242115│5.306258386E¯15│4.095E¯9│5.154759231 2.136173738 1.262178351 0.1063794478│\n",
       "└──┴──────────┴───────────────┴────────┴────────────────────────────────────────────────┘\n",
       "</pre>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg←1e3 ⎕CT ⎕CT 1e¯2 1e¯2 5(÷5)(÷⎕CT)⎕CT   ⍝ ti tc tr tg d0 di dd dx dn\n",
    "Verbose←{⎕←(3 0⍕⊃⍵),12 9∘⍕¨1↓⍵}\n",
    "cfg GaussEval1 LM Verbose(4⍴1)1\n",
    "cfg GaussEval2 LM Verbose(4⍴1)1\n",
    "cfg GaussEval3 LM Verbose(4⍴1)1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dyalog APL",
   "language": "apl",
   "name": "dyalog_apl"
  },
  "language_info": {
   "file_extension": ".apl",
   "mimetype": "text/apl",
   "name": "APL"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
